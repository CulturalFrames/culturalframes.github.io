<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics">
  <meta name="keywords" content="CulturalFrames, Text-to-Image, AI, Machine Learning, Benchmark, Multimodal, LLMs">
  <meta name="author" content="CulturalFrames Research Team">
  <title>CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <!-- Navigation -->
  <nav class="navbar">
    <div class="container">
      <div class="navbar-content">
        <a href="#" class="navbar-brand">CulturalFrames</a>
        <ul class="navbar-menu">
          <li><a href="#about" class="navbar-item">About</a></li>
          <li><a href="#news" class="navbar-item">News</a></li>
          <!-- <li><a href="#tasks" class="navbar-item">Tasks</a></li> -->
          <li><a href="#dataset" class="navbar-item">Dataset</a></li>
          <li><a href="#explore" class="navbar-item">Explore</a></li>
          <li><a href="#results" class="navbar-item">Results</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Hero Section -->
  <section class="hero hero-blend">
    <div class="floating-circle-1"></div>
    <div class="floating-circle-2"></div>
    <div class="floating-circle-3"></div>
    <div class="floating-square"></div>
    <div class="floating-triangle"></div>
    <div class="floating-diamond"></div>
    <div class="floating-symbols">● ○ ◇ □</div>
    <div class="floating-hexagon"></div>
    <div class="floating-star"></div>
    <div class="floating-oval"></div>
    <div class="floating-cross"></div>
    <div class="floating-wave"></div>
    <div class="floating-dots">•••</div>
    <div class="floating-arrow"></div>
    <div class="floating-pentagon"></div>
    <div class="container hero-flex">
      <div class="hero-left">
        <div class="hero-meta-badge-row">
          <div class="hero-multilingual-badge">Text-to-Image benchmark</div>
          <div class="hero-emnlp-badge">EMNLP 2025 Findings</div>
        </div>
        <h1 class="hero-title">
          CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics
        </h1>
        <div class="hero-authors-compact">
          <span>Shravan Nayak<sup>1,2*</sup></span>
          <span>Mehar Bhatia<sup>1,3</sup></span>
          <span>Xiaofeng Zhang<sup>1,2</sup></span>
          <span>Verena Rieser<sup>5</sup></span>
          <span>Lisa Anne Hendricks<sup>5</sup></span>
          <span>Sjoerd van Steenkiste<sup>4</sup></span>
          <span>Yash Goyal<sup>6</sup></span>
          <span>Karolina Stańczak<sup>1,3,7</sup></span>
          <span>Aishwarya Agrawal<sup>1,2</sup></span>
        </div>
        <div class="hero-affiliations-compact">
          <span><sup>1</sup>Mila - Quebec AI Institute</span>
          <span><sup>2</sup>Université de Montréal</span>
          <span><sup>3</sup>McGill University</span>
          <span><sup>4</sup>Google Research</span>
          <span><sup>5</sup>Google DeepMind</span>
          <span><sup>6</sup>Samsung - SAIT AI Lab, Montreal</span>
          <span><sup>7</sup>ETH AI Center</span>
        </div>
    
        <div class="hero-description">
          Do text-to-image models understand culture?
We test whether their images capture both the explicit details you type into the prompt and the implicit cultural cues you don't. Using a suite of culture-anchored scenes, we evaluate the images they produce and the metrics meant to score them and uncover a wide gap between model output and what people actually expect.
        </div>
        <div class="hero-buttons">
          <a href="https://www.arxiv.org/abs/2506.08835" class="hero-btn">📄 Paper</a>
          <a href="https://github.com/mair-lab/CulturalFrames" class="hero-btn">💻 Code</a>
          <a href="https://huggingface.co/datasets/mair-lab/CulturalFrames" class="hero-btn">📥 Dataset</a>
        </div>
        <em>*Corresponding: <a href="mailto:shravan.nayak@mila.quebec">shravan.nayak@mila.quebec</a></em>

      </div>
      <div class="hero-right">
        <img src="assets/cultural_frames_cover.png" alt="CulturalFrames Cover" class="hero-cover-image" />
      </div>
      
    </div>
  </section>

  <!-- News Section -->
  <section id="news" class="section">
    <div class="container">
      <h2 class="section-title">News</h2>
      <div class="news-container">
        <div class="news-item">
          <div class="news-date">August 2025</div>
          <div class="news-content">
            <strong>Paper accepted at EMNLP 2025 Findings!</strong> - See you in Suzhou, China!
          </div>
        </div>
        <div class="news-item">
          <div class="news-date">August 2025</div>
          <div class="news-content">
            <strong>Dataset and code release</strong> - CulturalFrames images and prompts released along with code for prompt/image generation and metric evaluation. Annotations will be released soon.
          </div>
        </div>
        <div class="news-item">
          <div class="news-date">June 2025</div>
          <div class="news-content">
            <strong>Paper released on arXiv:</strong> <a href="https://www.arxiv.org/abs/2506.08835" target="_blank">CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics</a>
          </div>
        </div>
        <div class="news-item">
          <div class="news-date">May 2025</div>
          <div class="news-content">
            <strong>CulturalFrames accepted at ICML 2025 Workshop on Models of Human Feedback for AI Alignment (MoFA)</strong>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- About Section -->
  <section id="about" class="section">
    <div class="container">
        <h2 class="section-title">About</h2>

      <p class="section-subtitle">
        CulturalFrames tackles a key question: can today's text-to-image models and the metrics that judge them meet our cultural expectations? Existing benchmarks fixate on literal prompt matching or concept-centric checks, overlooking the nuanced cultural context that guides human judgment—and our study shows this blind spot is wide. 
      </p>
     
  
      <div class="card">
      <ul class="features-list">
        <li></li>The increasing ubiquity of text-to-image (T2I) models as tools for visual content generation raises concerns about their ability to accurately represent diverse cultural contexts. In this work, we present the first study to systematically quantify the alignment of T2I models and evaluation metrics with respect to both explicit as well as implicit cultural expectations. To this end, we introduce CulturalFrames, a novel benchmark designed for rigorous human evaluation of cultural representation in visual generations. Spanning 10 countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts, 3,637 corresponding images generated by 4 state-of-the-art T2I models, and over 10k detailed human annotations.</li>
      </ul>
        <img src="assets/dataset.png" alt="CulturalFrames dataset overview" class="image image-large" />
        
        <h3 class="card-title">Key Features</h3>
        <ul class="features-list">
          <li><strong>🌍 Culturally Rich Prompts:</strong> We design a methodoly to generate scenarios grounded in real-life cultural practices across 10 countries, covering domains like family, dates-of-significance, etiquette, religion and greetings.</li>
          <li><strong>🎯 Implicit & Explicit Evaluation:</strong> We capture whether models reflect both directly stated (explicit) and contextually expected (implicit) cultural cues in generations.</li>
          <li><strong>🧑🏽‍🤝‍🧑🏾 Diverse Human Judgments:</strong> Over 10,000 human ratings and free-form rationales collected from annotators with cultural familiarity and training across 4 criteria - image-prompt alignment, image quality, stereotype presence and overall satisfaction.</li>
          <li><strong>📝 Word-Level Error Tagging:</strong> Annotators identify specific prompt words that are incorrectly visualized, enabling fine-grained diagnostic analysis of model behavior.</li>
          <li><strong>📊 Model & Metric Benchmarking:</strong> Evaluates both text-to-image generation and metric performance, revealing where models and metrics diverge from human expectations</li>
          <li><strong>💬 Explanation-Based Analysis:</strong> Natural language explanations for ratings provide deeper insight into model failures and cultural mismatches.</li>
          <li><strong>📂 Open & Reproducible:</strong> All prompts, ratings, explanations, and model outputs will be publicly released for transparent, community-driven evaluation.</li>
        </ul>
      </div>
    </div>
    </section>


    <!-- Dataset Section -->
  <section id="dataset" class="section">
    <div class="container">
        <h2 class="section-title">Dataset Overview</h2>
        <p class="section-subtitle-dataset-overview">
          CulturalFrames is built with rigorous quality standards to capture the richness and complexity of real-world cultural scenarios. 
        </p>
      
      <div class="card">
        <h3 class="card-title">Data Collection</h3>
        <p style="font-size:1.0rem;line-height:1.6;margin-bottom:2rem;">
          To construct our dataset, we first curated culturally grounded knowledge from five key categories—greetings, family structure, etiquette, religion, and dates of significance—using the Cultural Atlas database. We then prompted LLMs to ground this knowledge into culturally reflective image generation prompts. Each prompt was reviewed by three annotators from the respective country, and only prompts with majority agreement on their cultural appropriateness were retained. Images were generated using four state-of-the-art text-to-image models. To collect reliable annotations, we ran multiple pilot studies to refine the rating instructions, and implemented a quality control loop that included annotator filtering and continuous feedback for high-performing annotators. This process yields a culturally rich dataset with over 10,000 human ratings with scores and free-text rationales.
        </p>
        <div class="dataset-overview-flex" style="display: flex; flex-wrap: wrap; gap: 2.5rem; align-items: stretch; justify-content: center;">
          <!-- Left: Distribution Image -->
          <div class="dataset-overview-left" style="flex: 1 1 420px; min-width: 340px; max-width: 520px; display: flex; flex-direction: column; align-items: center; justify-content: flex-start;">
            <img src="assets/overall_prompt_distribution.png" alt="Prompt Distribution by Domain" style="width:100%; max-width:380px; min-width:260px; aspect-ratio:1/1; border-radius: 16px; box-shadow: 0 2px 12px rgba(30,41,59,0.10); background: #fff; border: 1px solid #e2e8f0; margin-bottom: 1.1em;" />
            <div class="dataset-image-caption" style="font-size:1.04em; color:#64748b; text-align:center; max-width:420px; margin-top:0.2em;">Distribution of prompts across five socio-cultural domains in CulturalFrames</div>
          </div>
          <!-- Right: Stats Table -->
          <div class="dataset-overview-right" style="flex: 1 1 340px; min-width: 300px; max-width: 420px; display: flex; align-items: center; justify-content: flex-start; flex-direction: column;">
            <div class="dataset-stats-grid">
              <div class="stat-card">
                <div class="stat-icon">📝</div>
                <div class="stat-number">983</div>
                <div class="stat-label">Cultural Prompts</div>
              </div>
              <div class="stat-card">
                <div class="stat-icon">🖼️</div>
                <div class="stat-number">3,637</div>
                <div class="stat-label">Generated Images</div>
              </div>
              <div class="stat-card">
                <div class="stat-icon">🤖</div>
                <div class="stat-number">4</div>
                <div class="stat-label">T2I Models</div>
              </div>
              <div class="stat-card">
                <div class="stat-icon">✍️</div>
                <div class="stat-number">10,000+</div>
                <div class="stat-label">Human Annotations</div>
              </div>
              <div class="stat-card">
                <div class="stat-icon">📋</div>
                <div class="stat-number">4</div>
                <div class="stat-label">Annotation Criteria</div>
              </div>
              <div class="stat-card">
                <div class="stat-icon">👥</div>
                <div class="stat-number">400+</div>
                <div class="stat-label">Participants</div>
              </div>
            </div>
            <div class="dataset-table-caption" style="font-size:1.04em; color:#64748b; text-align:center; max-width:420px; margin-top:0.9em;">Summary statistics for the CulturalFrames</div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Dataset Exploration Section -->
  <section id="explore" class="section">
    <div class="container">
      <h2 class="section-title">Explore CulturalFrames</h2>
      <p class="section-subtitle">
        Browse through samples from different countries in the CulturalFrames dataset.
      </p>
      
      <!-- Country Selection Tabs -->
      <div class="country-tabs">
        <button class="country-tab active" data-country="india">🇮🇳 India</button>
        <button class="country-tab" data-country="japan">🇯🇵 Japan</button>
        <button class="country-tab" data-country="china">🇨🇳 China</button>
        <button class="country-tab" data-country="iran">🇮🇷 Iran</button>
        <button class="country-tab" data-country="germany">🇩🇪 Germany</button>
        <button class="country-tab" data-country="poland">🇵🇱 Poland</button>
        <button class="country-tab" data-country="south-africa">🇿🇦 South Africa</button>
        <button class="country-tab" data-country="canada">🇨🇦 Canada</button>
        <button class="country-tab" data-country="brazil">🇧🇷 Brazil</button>
        <button class="country-tab" data-country="chile">🇨🇱 Chile</button>
      </div>

      <!-- Cultural Samples Display -->
      <div class="cultural-samples">
        <div class="sample-carousel">
          <button class="carousel-btn prev" data-task="cultural">‹</button>
          <div class="carousel-track" data-task="cultural">
            <!-- Cultural samples will be loaded here -->
          </div>
          <button class="carousel-btn next" data-task="cultural">›</button>
        </div>
      </div>

      <!-- Annotation Browsing Section -->
      <div class="annotation-section">
        <h3 class="annotation-subtitle">Browse CulturalFrames Annotations</h3>
        <p class="annotation-description">
          Explore human evaluations of cultural alignment, quality, stereotype and overall satisfaction across different countries.
        </p>
        
        <!-- Annotation Display -->
        <div class="annotation-samples">
          <div class="annotation-carousel">
            <button class="carousel-btn prev" data-task="annotation">‹</button>
            <div class="carousel-track" data-task="annotation">
              <!-- Annotation samples will be loaded here -->
            </div>
            <button class="carousel-btn next" data-task="annotation">›</button>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Results Section -->
  <section id="results" class="section">
    <div class="container">
      <h2 class="section-title">Results</h2>
      <p class="section-subtitle">
        Our evaluation reveals significant gaps in cultural alignment across both text-to-image models and evaluation metrics.
      </p>
      
      <!-- Models Subsection -->
      <div class="results-subsection">
        <h3 class="subsection-title">Models Performance</h3>
        
        <!-- Model Summary Scores -->
        <div class="card">
          <h4 class="card-title">Model Performance Summary</h4>
          <p class="card-description">
            Human evaluations show GPT-Image leads in prompt alignment (0.85) and overall preference, with Imagen3 rated highest for image quality. Open-source models SD-3.5-Large and Flux lag, with SD performing worst due to low quality and higher stereotype rates. Stereotypical outputs occur in 10-16% of images, most for SD and least for Flux. Cross-country differences are notable, with Asian countries giving lower scores. Of all sub-perfect ratings, 50.3% are explicit errors, 31.2% implicit, and 17.9% both, showing persistent cultural nuance challenges.
          </p>
          <div class="asymmetric-graph-container">
            <div class="asymmetric-graph-large">
              <img src="assets/combined_metrics.png" alt="Combined Metrics Across Countries" class="image"
                   style="max-width: 100%; height: auto; background: #f8f9fa; border: 2px dashed #ddd; padding: 0.8rem; text-align: center;"
                   onerror="this.innerHTML='<div style=&quot;text-align:center;padding:2rem;color:#666;font-size:1rem;&quot;>📊 Combined Metrics<br><small>(Coming Soon)</small></div>'">
              <div class="graph-caption">Human evaluation results for selected T2I models. From left to right: 1) Prompt Alignment (0-1 scale). 2) Image Quality (0-1 scale). 3) Stereotype Score (0-1 scale, 0 indicates no stereotyping). 4) Overall Score (1-5 Likert scale).</div>
            </div>
            <div class="asymmetric-graph-small">
              <img src="assets/stacked_error_distribution.png" alt="Error Distribution Across Countries" class="image"
                   style="max-width: 100%; height: auto; background: #f8f9fa; border: 2px dashed #ddd; padding: 0.8rem; text-align: center;"
                   onerror="this.innerHTML='<div style=&quot;text-align:center;padding:2rem;color:#666;font-size:1rem;&quot;>📈 Error Distribution<br><small>(Coming Soon)</small></div>'">
              <div class="graph-caption">Distribution of image-prompt alignment errors (score < 1) by model, grouped by error type.</div>
            </div>
          </div>
        </div>
        
        <!-- Cross-Country Disparity -->
        <div class="card">
          <h4 class="card-title">Cultural Disparity Across Countries</h4>
          <p class="card-description">
            There is a disparity in model performance across countries, for different criteria. Images generated for Asian countries such as Japan and Iran generally have lower scores across all criteria. The plots below show performance metrics and error distributions across countries.
          </p>
          <div class="graph-placeholder">
            <img src="assets/country_wise_scores_pa.png" alt="Model Disparity Across Countries" class="image image-large"
                 style="max-width: 100%; height: auto; background: #f8f9fa; border: 2px dashed #ddd; padding: 0.6rem; text-align: center;"
                 onerror="this.innerHTML='<div style=&quot;text-align:center;padding:2rem;color:#666;font-size:1.1rem;&quot;>📈 Cross-Country Disparity Graph<br><small>(Coming Soon)</small></div>'">
          </div>
        </div>
        
        <!-- Word Misinterpretation Analysis -->
        <div class="card">
          <h4 class="card-title">Word Misinterpretation Analysis</h4>
          <p class="card-description">
            Words highlighted as problematic in prompts flagged by raters highlights two main error patterns. Country demonyms (e.g., Iranian, Brazilian) are often marked when an image lacks the expected country-specific element or when annotators cannot relate to its content. Other frequent errors involve broad cultural signifiers—such as rituals, social roles, and iconic objects—showing that T2I models often misrepresent these elements.
          </p>
          <div class="graph-placeholder">
            <img src="assets/word_frequency.png" alt="Word Misinterpretation Frequency Analysis" class="image"
                 style="max-width: 100%; max-height: 700px; width: auto; height: auto; background: #f8f9fa; border: 2px dashed #ddd; padding: 0.6rem; text-align: center; object-fit: contain;"
                 onerror="this.innerHTML='<div style=&quot;text-align:center;padding:2rem;color:#666;font-size:1.1rem;&quot;>📊 Word Frequency Analysis<br><small>(Coming Soon)</small></div>'">
          </div>
        </div>
        
        <!-- Model Failure Cases -->
        <!-- <div class="card">
          <h4 class="card-title">Model Failure Cases</h4>
          <p class="card-description">
            Explore specific examples where different models fail to capture cultural expectations.
          </p>
          
          <!-- Model Selection Tabs -->
          <!-- <div class="model-tabs">
            <button class="model-tab active" data-model="gptimage">GPT-Image</button>
            <button class="model-tab" data-model="imagen3">Imagen3</button>
            <button class="model-tab" data-model="flux1dev">Flux.1-Dev</button>
            <button class="model-tab" data-model="sd35large">SD-3.5 Large</button>
          </div> -->
          
          <!-- Model Failure Display -->
          <!-- <div class="model-failures">
            <div class="model-failure-carousel">
              <button class="carousel-btn prev" data-task="model-failure">‹</button>
              <div class="carousel-track" data-task="model-failure">
                <!-- Model failure samples will be loaded here -->
              <!-- </div>
            </div>
          </div>
        </div> -->
      </div>
      
      <!-- Metrics Subsection -->
      <div class="results-subsection">
        <h3 class="subsection-title">Evaluation Metrics Analysis</h3>
        
        <!-- Metric Performance -->
        <div class="card">
          <h4 class="card-title">Metric Alignment with Human Judgment</h4>
          <p class="card-description">
            Metrics correlate poorly with human judgments. VIEScore and UnifiedReward show the strongest alignment, though still below human-human agreement. All metrics perform poorly on image quality. Overall, VLM-based metrics best capture culturally grounded preferences.
          </p>
          <div class="graph-placeholder">
            <img src="assets/human_correlation.png" alt="Metric Alignment Analysis" class="image"
                 style="max-width: 100%; max-height: 400px; width: auto; height: auto; background: #f8f9fa; border: 2px dashed #ddd; padding: 0.6rem; text-align: center; object-fit: contain;"
                 onerror="this.innerHTML='<div style=&quot;text-align:center;padding:2rem;color:#666;font-size:1rem;&quot;>📏 Metric Alignment Graph<br><small>(Coming Soon)</small></div>'">
          </div>
        </div>
        
        <!-- Metric Failure Cases -->
        <div class="card">
          <h4 class="card-title">Metric vs Human Evaluation Discrepancies</h4>
          <p class="card-description">
            The reasons provided by automatic metrics are often not aligned with human judgments. Below we present some examples where automated evaluation metrics disagree with human cultural assessment.
          </p>
          
          <!-- Metric Failure Display -->
          <div class="metric-failures">
            <div class="metric-failure-carousel">
              <button class="carousel-btn prev" data-task="metric-failure">‹</button>
              <div class="carousel-track" data-task="metric-failure">
                <!-- Metric failure samples will be loaded here -->
              </div>
              <button class="carousel-btn next" data-task="metric-failure">›</button>
            </div>
          </div>
        </div>
        
        <!-- Discussion -->
        
          <h3 class="subsection-title">Improving Models and Metrics</h3>
          <div class="card">
          <p class="card-description">
            Based on our analysis of cultural misalignment in text-to-image models and their evaluation metrics, we highlight three key directions for improvement.
          </p>
          
          <div class="discussion-section">
            <h5 class="discussion-subheading">📝 Prompt Enhancement</h5>
            <p class="discussion-content">
              We expand culturally implicit prompts in CulturalFrames by automatically adding missing cues (cultural objects, family roles, setting details, mood/atmosphere) based on our analysis of model failures. We generate images for the new prompts using Flux.1-Dev and evaluate alignment with VIEScore. This targeted, culturally informed expansion improves VIEScore from 7.3 for original prompts to 8.4 for the expanded prompts, showing that making implicit cues explicit helps better image generation.
            </p>
          </div>
          
          <div class="discussion-section">
            <h5 class="discussion-subheading">📏 Metric Design</h5>
            <p class="discussion-content">
              We rewrote VIEScore's GPT-4o instructions using our human rater guidelines to make both implicit and explicit cues salient, then re-evaluated image-prompt alignment. This raised Spearman correlation with human ratings from 0.30 to 0.32 and improved explanation alignment from 2.19 to 2.37 (5-point scale). Carefully crafted, culturally informed instructions thus boost both scores and rationales.
            </p>
          </div>
          
          <div class="discussion-section">
            <h5 class="discussion-subheading">🎯 Metric Training</h5>
            <p class="discussion-content">
              We compare a preference-trained judge (UnifiedReward on Qwen2.5-VL-7B) to its backbone and find consistently higher correlations with human judgments. It even edges out GPT-4o-based VIEScore on alignment (0.31 vs 0.30). Preference-based judge training, even without culture-specific data, meaningfully improves cultural alignment of metric scores and CulturalFrames can be used to push this further.
            </p>
          </div>
        </div>
      </div>
      
    </div>
  </section>

    <!-- BibTeX Section -->
  <section class="section">
    <div class="container">
      <h2 class="section-title">Citation</h2>
      <div class="bibtex-block">
@inproceedings{nayak2025culturalframes,
  title={CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics},
  author={Shravan Nayak, Mehar Bhatia, Xiaofeng Zhang, Verena Rieser, Lisa Anne Hendricks, Sjoerd van Steenkiste, Yash Goyal, Karolina Stańczak, Aishwarya Agrawal},
  booktitle={ICML 2025 Workshop on Models of Human Feedback for AI Alignment (MoFA)},
  year={2025}
}
      </div>
    </div>
    </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <p>&copy; 2025 CulturalFrames Benchmark. This website has been adapted from the WebMMU website created by Rabiul Awal and redesigned with the help of Claude.</p>
      <p>
        <!-- <a href="" target="_blank">GitHub</a> •  -->
        <a href="mailto:shravan.nayak@mila.quebec">Contact</a> • 
        <a href="https://www.arxiv.org/abs/2506.08835" target="_blank">arXiv</a>
      </p>
    </div>
  </footer>

  <script src="js/cultural-data.js"></script>
  <script src="js/annotation-data.js"></script>
  <script src="js/metric-failure-data.js"></script>
  <script src="js/script.js"></script>
</body>
</html>
